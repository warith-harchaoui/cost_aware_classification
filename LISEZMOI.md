# üöÄ Classification Sensible au Co√ªt + Benchmark Fraude (IEEE-CIS)

**Auteur :** Warith Harchaoui <wharchaoui@nexton-group.com>

D√©p√¥t de recherche et d'ing√©nierie d'entreprise pour la **classification sensible au co√ªt** avec des **co√ªts de mauvaise classification d√©pendants de l'exemple**. Cette bo√Æte √† outils transforme l'apprentissage automatique traditionnel du "simple rapprochement d'√©tiquettes" √† la "**maximisation du profit commercial**".

## üéØ Le Probl√®me M√©tier

La classification traditionnelle (Entropie Crois√©e) traite toutes les erreurs comme √©gales. Dans le monde r√©el, **certaines erreurs sont beaucoup plus co√ªteuses que d'autres** :
- **Faux D√©clin :** Refuser un client l√©gitime co√ªte la marge de la transaction + la frustration du client + une possible perte de client√®le (churn).
- **Fausse Approbation (Fraude) :** Accepter une carte vol√©e co√ªte le montant total de la transaction + les frais de r√©trofacturation (chargeback) + les co√ªts op√©rationnels.

Ce d√©p√¥t impl√©mente des fonctions de perte bas√©es sur le **Transport Optimal (OT)** qui "comprennent" ces co√ªts pendant l'entra√Ænement, permettant aux mod√®les de prendre des d√©cisions qui minimisent le regret financier plut√¥t que de simplement compter les erreurs.

---

## üìç Table des Mati√®res

- [Le Probl√®me M√©tier](#-le-probl√®me-m√©tier)
- [D√©marrage Rapide pour les D√©cideurs](#-d√©marrage-rapide-pour-les-d√©cideurs)
- [Fonctions de Perte Disponibles](#-fonctions-de-perte-disponibles)
  - [Pertes de R√©f√©rence (Baselines)](#1-pertes-de-r√©f√©rence-baselines)
  - [Pertes Sensibles au Co√ªt (Transport Optimal)](#2-pertes-sensibles-au-co√ªt-transport-optimal)
- [Guide de R√©glage d'Epsilon (Œµ)](#Ô∏è-guide-de-r√©glage-depsilon-Œµ)
- [Conseils de Performance](#-conseils-de-performance)
- [Guide d'Utilisation Complet](#-guide-dutilisation-complet)
- [M√©triques et √âvaluation](#-m√©triques-et-√©valuation)
- [Choisir une Fonction de Perte](#-choisir-une-fonction-de-perte)
- [Documentation et Ressources](#-documentation)
- [Tests](#-tests)
- [Citation](#-citation)
- [Licence](#-licence)

---

## üí° D√©marrage Rapide pour les D√©cideurs

Si vous souhaitez voir imm√©diatement l'impact m√©tier, lancez le benchmark complet :

```bash
# Comparer toutes les pertes par rapport aux r√©f√©rences financi√®res
python -m examples.fraud_detection --loss all --epochs 15 --run-id impact_metier
```

**Ce qu'il faut regarder dans les r√©sultats :**
- **Regret R√©alis√© :** L'argent r√©ellement perdu en production.
- **Regret Optimal Attendu :** La perte th√©orique minimale possible.
- **R√©f√©rence Na√Øve (Naive Baseline) :** Ce qui se passe si vous faites simplement "Tout Approuver" ou "Tout Refuser".

---

## üìã Fonctions de Perte Disponibles

### Pertes de R√©f√©rence (Baselines)

#### 1. **Entropie Crois√©e** (`cross_entropy`)
Perte d'entropie crois√©e standard sans sensibilisation au co√ªt.

**Quand l'utiliser :** Comparaison de base lorsque toutes les mauvaises classifications ont un co√ªt √©gal.

#### 2. **Entropie Crois√©e Pond√©r√©e** (`cross_entropy_weighted`)
Entropie crois√©e pond√©r√©e par √©chantillon avec des poids $w_i = C_i[y_i, 1-y_i]$ d√©riv√©s de la matrice de co√ªt.

**Quand l'utiliser :** Baseline simple sensible au co√ªt qui repond√®re les exemples par leur co√ªt de mauvaise classification.

### Pertes Sensibles au Co√ªt (Transport Optimal)

Toutes les pertes bas√©es sur l'OT utilisent une matrice de co√ªt $C$ o√π $C_{ij}$ repr√©sente le co√ªt de pr√©dire la classe $j$ quand la classe r√©elle est $i$.

#### Comprendre la R√©gularisation Epsilon (Œµ)

Le param√®tre de r√©gularisation entropique Œµ contr√¥le la fluidit√© du transport optimal. **Par d√©faut, Œµ est calcul√© de mani√®re adaptative √† partir de la matrice de co√ªt.**

**Avantages d'Œµ adaptatif :**
- S'adapte automatiquement √† l'ampleur de votre matrice de co√ªt.
- Aucun r√©glage manuel requis.
- Robuste √† travers diff√©rents domaines.

#### 3. **Perte Sinkhorn-Fenchel-Young** (`sinkhorn_fenchel_young`)
Utilise le th√©or√®me de l'enveloppe pour des gradients stables. Id√©al pour une diff√©renciation implicite.

#### 4. **Perte Sinkhorn Envelope** (`sinkhorn_envelope`)
Impl√©mentation personnalis√©e avec gradients d'enveloppe. Efficace en m√©moire et stable.

#### 5. **Perte Sinkhorn Full Autodiff** (`sinkhorn_autodiff`)
Diff√©renciation compl√®te √† travers toutes les it√©rations de Sinkhorn. Plus "bout-en-bout" mais consomme plus de m√©moire.

#### 6. **Perte Sinkhorn POT** (`sinkhorn_pot`) ‚≠ê
Utilise la biblioth√®que reconnue [Python Optimal Transport (POT)](https://pythonot.github.io/).
- **Recommand√© pour la production.**
- Meilleure stabilit√© num√©rique.

---

## üéõÔ∏è Guide de R√©glage d'Epsilon (Œµ)

Le param√®tre `--epsilon-scale` multiplie l'Œµ adaptatif :
- **< 1.0 :** R√©gularisation plus serr√©e, d√©cisions plus tranch√©es.
- **= 1.0 :** √âquilibre par d√©faut.
- **> 1.0 :** R√©gularisation plus souple, solutions plus robustes face au bruit.

---

## üìä M√©triques et √âvaluation

Pour mesurer r√©ellement le succ√®s commercial, nous allons au-del√† de la Pr√©cision et de l'AUC-ROC :

- **PR-AUC (Aire sous la courbe Pr√©cision-Rappel) :** M√©trique principale pour les donn√©es de fraude d√©s√©quilibr√©es (**plus c'est haut, mieux c'est**).
- **Luck Baseline :** Ligne horizontale repr√©sentant un classificateur al√©atoire.
- **Regret Optimal Attendu :** Co√ªt m√©tier attendu si nous prenons la d√©cision math√©matiquement optimale (**plus c'est bas, mieux c'est**).
- **Regret R√©alis√© :** L'argent r√©ellement perdu. Inclut les pertes dues √† la fraude accept√©e et le manque √† gagner des faux d√©clins.
- **R√©f√©rence Na√Øve :** Compare la meilleure des strat√©gies simples ("Tout Approuver" ou "Tout Refuser"). **Votre mod√®le doit battre cette r√©f√©rence pour √™tre utile.**

---

## ‚ö° Conseils de Performance

### Chargement de Donn√©es Robuste
Nous recommandons l'utilisation du moteur Python pour lire les fichiers CSV volumineux d'IEEE-CIS afin d'√©viter les erreurs `ParserError`.

### Stabilit√© Num√©rique
- **`RobustScaler`** : Pour g√©rer les valeurs aberrantes des montants.
- **`CosineAnnealingLR`** : Pour une convergence plus fluide.
- **Taux d'Apprentissage Faibles** : Commencer √† `1e-5` pour des gradients plus stables.

---

## üéØ Choisir une Fonction de Perte

| Perte | Avantages | Inconv√©nients | Id√©al pour |
|-------|-----------|---------------|------------|
| `cross_entropy` | Simple, rapide | Ignore les co√ªts | Comparaison de base |
| `sinkhorn_pot` | **Pr√™t pour la production** | D√©pendance externe | D√©ploiements r√©els ‚≠ê |
| `sinkhorn_envelope` | Stable, peu de m√©moire | Impl√©mentation maison | M√©moire limit√©e |

---

## ‚úçÔ∏è Citation

Si vous utilisez ce travail, merci de citer :

```bibtex
@inproceedings{harchaoui2026cacis,
  title={Cost-Aware Classification with Optimal Transport for E-commerce Fraud Detection},
  author={Harchaoui, Warith and Pantanacce, Laurent},
  booktitle={The 32nd ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '26)},
  year={2026}
}
```

## üìú Licence

**Unlicense** ‚Äî Ce logiciel est libre et appartient au domaine public.  
Voir [UNLICENSE](unlicense.org) pour plus de d√©tails.
