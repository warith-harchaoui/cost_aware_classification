\documentclass[sigconf]{acmart}

% =========================================================
% PACKAGES
% =========================================================
\usepackage{amsmath, amsfonts, amsthm}
\usepackage{mathtools, physics, bm, microtype}
\usepackage{booktabs, enumitem, algorithm, algorithmic}
\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning, shapes.geometric, calc}

% =========================================================
% VISIBLE TODOs (no extra packages)
% =========================================================
\newcommand{\TODO}[1]{\textbf{[TODO: #1]}}

% =========================================================
% NOTATION MACROS
% =========================================================
\newcommand{\CACIS}{\textsc{CACIS}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vy}{\mathbf{y}}
\newcommand{\vz}{\mathbf{z}}
\newcommand{\vp}{\mathbf{p}}
\newcommand{\valpha}{\bm{\alpha}}
\newcommand{\mC}{\mathbf{C}}
\newcommand{\mM}{\mathbf{M}}
\newcommand{\mV}{\mathbf{V}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\OT}{\mathrm{OT}}
\newcommand{\Sink}{\mathrm{Sink}}
\newcommand{\vone}{\mathbf{1}}
\newcommand{\ve}{\mathbf{e}}
\newcommand{\DeltaK}{\Delta^K}
\newcommand{\KL}{\mathrm{KL}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\FW}{\mathrm{FW}}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\diag}{diag}

% =========================================================
% KDD METADATA
% =========================================================
\copyrightyear{2026}
\acmYear{2026}
\setcopyright{acmcopyright}
\acmConference[KDD '26]{The 32nd ACM SIGKDD Conference on Knowledge Discovery and Data Mining}{August 2026}{Paris, France}

\begin{document}

\title{Cost-Aware Classification with Optimal Transport \\for E-commerce Fraud Detection}

\author{Warith Harchaoui}
\affiliation{\institution{NEXTON} \city{Paris} \country{France}}
\email{wharchaoui@nexton-group.com}

\author{Laurent Pantanacce}
\affiliation{\institution{NEXTON} \city{Paris} \country{France}}
\email{lpantanacce@nexton-group.com}

% =========================================================
% ABSTRACT
% =========================================================
\begin{abstract}
Industrial classifiers rarely operate in isolation: predictions trigger decisions with asymmetric consequences.
Yet the dominant training objective, cross-entropy, is decision-agnostic---it treats all misclassifications as equally undesirable and ignores the geometry induced by business values on the label set.
We introduce \CACIS{} (Cost-Aware Classification with Informative Selection), a differentiable Fenchel--Young loss generated by a Sinkhorn/Optimal-Transport regularizer.
\CACIS{} internalizes operational regret into the networkâ€™s geometry by replacing the ``flat'' label space of cross-entropy with a task-specific cost geometry.
We provide (i) a decision-theoretic framing of cost-aware classification \citep{berger1985statistical}, (ii) an efficient optimization procedure based on a simplex-stable Frank--Wolfe inner loop, and (iii) evidence on the IEEE-CIS/Vesta e-commerce fraud benchmark \citep{kaggle_ieee_cis_2019}.
On \TODO{chronological split details}, we reduce expected regret by \TODO{X\%} at matched \TODO{approval rate / fraud catch rate}, while maintaining strong AUC-ROC and calibration.
\end{abstract}

\maketitle

% =========================================================
% 1. INTRODUCTION
% =========================================================
\section{Introduction}
Fraud detection systems operate inside decision pipelines: a model output triggers an action (approve, decline, step-up), which triggers downstream outcomes with non-symmetric business consequences.
In such settings, accuracy is often a poor proxy for delivered value.
A false negative can cause direct losses, while a false positive reduces conversion and customer lifetime value.

Despite this, modern classifiers are typically trained with cross-entropy (negative log-likelihood), a strictly proper scoring rule \citep{gneiting2007strictly}.
Cross-entropy is decision-agnostic: it does not encode the cost geometry that differentiates types of mistakes, nor does it naturally model instance-dependent costs (e.g., high-amount transactions).

\paragraph{Key claim.}
Classification in decision systems is fundamentally a decision problem under uncertainty:
the relevant objective is expected regret (or profit), not a surrogate notion of correctness \citep{berger1985statistical}.

\paragraph{Why not just threshold or reweight?}
A common strategy is to train with cross-entropy and tune thresholds or post-hoc policies to account for costs.
This preserves a mismatch: the representation and probability landscape were shaped without the cost geometry.
Weighted cross-entropy or focal-style losses address imbalance, but still rely on a flat label geometry and cannot directly express structured or instance-dependent costs.
This paper instead aligns the \emph{training geometry} with the \emph{decision geometry}.

\paragraph{Contributions.}
\begin{itemize}[leftmargin=*]
    \item \textbf{Decision geometry.} We formalize cost-aware classification as decision-making on the simplex, where a cost matrix induces polyhedral decision regions.
    \item \textbf{\CACIS{} loss.} We introduce \CACIS{}, a cost-aware Fenchel--Young loss induced by a Sinkhorn/OT regularizer \citep{cuturi2013sinkhorn, benamou2015iterative, peyre2019computational}.
    \item \textbf{Optimization.} We provide a stable inner solver based on Frank--Wolfe on the simplex, avoiding numerical issues from unconstrained parameterizations.
    \item \textbf{Evidence on IEEE-CIS.} We evaluate on the IEEE-CIS/Vesta fraud benchmark \citep{kaggle_ieee_cis_2019} using chronological splits and decision-aligned regret/profit metrics in addition to AUC and calibration.
\end{itemize}

\paragraph{Roadmap.}
Section~\ref{sec:decision} introduces regret geometry and Bayes-optimal decisions.
Section~\ref{sec:ot} reviews OT/Sinkhorn geometry on label distributions.
Section~\ref{sec:cacis} presents \CACIS{} and its simplex-stable solver.
Section~\ref{sec:experiments} details the IEEE-CIS protocol and results.

% =========================================================
% 2. BACKGROUND: DECISION THEORY + REGRET GEOMETRY
% =========================================================
\section{The Geometry of Regret}
\label{sec:decision}

\subsection{From values to regret (cost) matrices}
Let $\mathcal{Y}=\{1,\dots,K\}$ be labels.
For clarity we assume the action set coincides with labels (extensions to richer actions are straightforward).
A value matrix $\mV \in \R^{K \times K}$ encodes $V_{ij}=$ value when truth is $i$ and decision is $j$.
We convert value to regret (cost) by defining, for each $i$,
\[
j^\star(i) \in \argmax_{j} V_{ij},
\qquad
C_{ij} = V_{i j^\star(i)} - V_{ij} \ge 0,
\]
so that $C_{i j^\star(i)}=0$ and costs share interpretable units (e.g., dollars).
This reduction is standard in statistical decision theory \citep{berger1985statistical}. Most of the time, $j^\star(i)=i$ which makes a zero-diagonal regret/cost matrix.

\subsection{Binary fraud and instance-dependent costs}
On IEEE-CIS, labels are binary: $\mathcal{Y}=\{\text{legit},\text{fraud}\}$ \citep{kaggle_ieee_cis_2019}.
To connect to business consequences, we design per-transaction costs using the transaction amount $M$ (feature \texttt{TransactionAmt}).
A stylized value matrix for approve/decline is:

\begin{table}[!h]
\centering
\caption{Template value matrix for binary fraud decisions (approve/decline). $M$ is transaction amount; $L_{\mathrm{fraud}}(M)$ is expected fraud loss if approved; $\rho_{\mathrm{FD}}$ models friction/foregone margin from declining a legitimate transaction.}
\label{tab:value}
\begin{tabular}{lcc}
\toprule
\textbf{Reality} $\backslash$ \textbf{Action} & \textbf{approve} & \textbf{decline} \\
\midrule
legit & $M$ & $-\rho_{\mathrm{FD}}\,M$ \\
fraud & $-L_{\mathrm{fraud}}(M)$ & $0$ \\
\bottomrule
\end{tabular}
\end{table}

This induces a regret matrix:

\begin{table}[!h]
\centering
\caption{Induced regret/cost matrix from Table~\ref{tab:value}. Costs are instance-dependent through $M$.}
\label{tab:cost}
\begin{tabular}{lcc}
\toprule
\textbf{Reality} $\backslash$ \textbf{Action} & \textbf{approve} & \textbf{decline} \\
\midrule
legit & $0$ & $(1+\rho_{\mathrm{FD}})M$ \\
fraud & $L_{\mathrm{fraud}}(M)$ & $0$ \\
\bottomrule
\end{tabular}
\end{table}

In experiments we instantiate $L_{\mathrm{fraud}}(M)$ via a multiplier,
e.g. $L_{\mathrm{fraud}}(M)=\lambda_{\mathrm{cb}}\,M$ with $\lambda_{\mathrm{cb}}\ge 1$
to capture expected chargeback overhead \TODO{state $\lambda_{\mathrm{cb}}$ and $\rho_{\mathrm{FD}}$}.

\subsection{Bayes-optimal decision rule under costs}
A probabilistic classifier outputs $\vp(\vx)\in\DeltaK$.
Given costs $\mC$, the expected cost of choosing action $j$ is
\begin{equation}
r(j \mid \vp) = \sum_{i=1}^K p_i C_{ij}.
\label{eq:risk_action}
\end{equation}
The Bayes-optimal decision is
\begin{equation}
\hat{y}(\vp) = \argmin_{j\in\mathcal{Y}} r(j \mid \vp).
\label{eq:bayes_decision}
\end{equation}
Unlike $\argmax_i p_i$, this rule depends on the entire distribution and the cost geometry.

\subsection{Decision regions in the simplex}
Fix $\mC$. The map $\vp \mapsto \hat{y}(\vp)$ partitions $\DeltaK$ into polyhedral regions defined by linear inequalities
$r(j\mid\vp) \le r(\ell\mid\vp)$.
This yields a \emph{decision geometry} that standard cross-entropy training does not encode.

% =========================================================
% 3. OT/SINKHORN LOSSES FOR LABEL GEOMETRY
% =========================================================
\section{Geometric Losses via Optimal Transport}
\label{sec:ot}

\subsection{Optimal Transport on label distributions}
A cost matrix $\mC$ can be interpreted as a ground cost between labels:
moving probability mass from label $i$ to $j$ costs $C_{ij}$.
Given distributions $\vp,\vq\in\DeltaK$, discrete OT is
\begin{equation}
\OT_{\mC}(\vp,\vq) = \min_{\bm{\pi}\in\R_{\ge 0}^{K\times K}}
\langle \bm{\pi}, \mC \rangle
\quad \text{s.t.}\quad
\bm{\pi}\vone=\vp,\;\bm{\pi}^\top \vone=\vq,
\label{eq:ot_primal}
\end{equation}
where $\langle \bm{\pi}, \mC\rangle=\sum_{i,j}\pi_{ij}C_{ij}$ \citep{villani2008optimal, peyre2019computational}.

\subsection{Entropic regularization and Sinkhorn}
To obtain smoothness and fast computation, we use entropic OT:
\begin{equation}
\OT^{\varepsilon}_{\mC}(\vp,\vq) =
\min_{\bm{\pi}\ge 0}
\langle \bm{\pi}, \mC \rangle
+ \varepsilon \sum_{i,j}\pi_{ij}(\log \pi_{ij}-1)
\quad \text{s.t.}\quad
\bm{\pi}\vone=\vp,\;\bm{\pi}^\top \vone=\vq.
\label{eq:entropic_ot}
\end{equation}
This yields Sinkhorn iterations via matrix scaling \citep{cuturi2013sinkhorn, benamou2015iterative}.
Sinkhorn divergences debias entropic OT and interpolate between OT and kernel discrepancies \citep{genevay2018learning, feydy2019interpolating, genevay2019sample}.
Unbalanced variants exist when mass is not preserved \citep{chizat2018unbalanced}.

\subsection{OT losses for classification: what they do and do not do}
For a one-hot label $\ve_y$, the OT loss simplifies:
\[
\OT_{\mC}(\ve_y,\vp) = \sum_{j=1}^K p_j C_{y j},
\]
a linear expected cost under randomized decision $j\sim\vp$.
This is cost-aware, but can be too linear to inherit the stability and calibration behavior associated with log-likelihood training \citep{gneiting2007strictly}.
This motivates a middle path: keep a smooth probabilistic training signal while injecting a non-trivial cost geometry.

% =========================================================
% 4. CACIS
% =========================================================
\section{\CACIS{}: Cost-Aware Classification with Informative Selection}
\label{sec:cacis}

\subsection{Fenchel--Young losses (reminder)}
Let $\Omega:\DeltaK\to\R$ be a strictly convex regularizer on the simplex.
The Fenchel conjugate is
\[
\Omega^*(\vz) = \sup_{\valpha\in\DeltaK}\ \langle \valpha,\vz\rangle - \Omega(\valpha),
\]
and the Fenchel--Young loss is
\begin{equation}
\ell_{\Omega}(y,\vz) = \Omega^*(\vz) - z_y + \Omega(\ve_y).
\label{eq:fy_loss}
\end{equation}
A key property is the universal gradient form
\begin{equation}
\nabla_{\vz}\ell_{\Omega}(y,\vz) = q(\vz)-\ve_y,
\qquad
q(\vz)=\nabla \Omega^*(\vz)\in\DeltaK.
\label{eq:fy_grad}
\end{equation}
Thus the training signal looks like cross-entropy (predicted distribution minus one-hot), but with a different geometry.

\subsection{Sinkhorn negentropy as a cost-aware regularizer}
Cross-entropy corresponds to choosing $\Omega$ as (negative) Shannon entropy, yielding the softmax map and KL geometry.
We replace Shannon geometry with a Sinkhorn/OT geometry inspired by geometric losses \citep{mensch2019geometric} and OT practice \citep{peyre2019computational, flamary2021pot}.

Define the \emph{Sinkhorn negentropy}:
\begin{equation}
\Omega_{\mC,\varepsilon}(\valpha)
\;=\;
-\frac{1}{2}\,\OT^{\varepsilon}_{\mC}(\valpha,\valpha).
\label{eq:sinkhorn_negentropy}
\end{equation}

\paragraph{\CACIS{} loss.}
\begin{equation}
\ell_{\CACIS}(y,\vz;\mC,\varepsilon) \;=\; \ell_{\Omega_{\mC,\varepsilon}}(y,\vz).
\end{equation}

\subsection{A variational form and the CACIS kernel}
For this choice of $\Omega$, one can derive a variational form:
\begin{equation}
\Omega_{\mC,\varepsilon}^*(\vz)
=
-\varepsilon \log\!\left(
\min_{\valpha\in\DeltaK}
\valpha^\top \mM(\vz,\mC,\varepsilon)\,\valpha
\right),
\label{eq:omega_star_variational}
\end{equation}
where $\mM(\vz,\mC,\varepsilon)\in\R^{K\times K}$ has entries
\begin{equation}
M_{ij}
=
\exp\!\left(
-\frac{z_i + z_j + C_{ij}}{\varepsilon}
\right).
\label{eq:M_entries}
\end{equation}
This ``CACIS kernel'' mixes scores and pairwise costs.

\subsection{Inner optimization via Frank--Wolfe on the simplex}
To evaluate~\eqref{eq:omega_star_variational}, we solve
\begin{equation}
\min_{\valpha\in\DeltaK}\ \mathcal{G}(\valpha),
\qquad
\mathcal{G}(\valpha) = \valpha^\top \mM \valpha.
\label{eq:inner_fw_problem}
\end{equation}
We use Frank--Wolfe because it is numerically stable on $\DeltaK$ and has a cheap linear oracle.

\begin{algorithm}[t]
\caption{Frank--Wolfe inner loop for \CACIS{}}
\label{alg:fw}
\begin{algorithmic}[1]
\STATE \textbf{Input:} logits $\vz$, cost matrix $\mC$, temperature $\varepsilon$, iterations $T$
\STATE Construct $\mM$ by \eqref{eq:M_entries}
\STATE Initialize $\valpha^{(0)}\leftarrow \vone/K$
\FOR{$t=0$ to $T-1$}
    \STATE $\mathbf{g}^{(t)} \leftarrow 2\,\mM \valpha^{(t)}$
    \STATE $k^\star \leftarrow \argmin_{k} g^{(t)}_k$ \hfill (linear oracle on simplex)
    \STATE $\gamma_t \leftarrow \frac{2}{t+2}$
    \STATE $\valpha^{(t+1)} \leftarrow (1-\gamma_t)\valpha^{(t)} + \gamma_t\,\ve_{k^\star}$
\ENDFOR
\STATE \textbf{Output:} $\valpha^\star \approx \valpha^{(T)}$
\end{algorithmic}
\end{algorithm}

\paragraph{Practical backpropagation.}
In our implementation we treat the Frank--Wolfe solution as a stable inner solver and do not backpropagate through FW iterations.
We compute $q(\vz)$ using \TODO{your exact mapping from $\valpha^\star$ to $q(\vz)$}, which preserves stability and keeps the outer-loop gradient in the form \eqref{eq:fy_grad}.

% =========================================================
% 5. EXPERIMENTS (IEEE-CIS)
% =========================================================
\section{Experimental Evaluation}
\label{sec:experiments}

\subsection{Dataset and temporal protocol (IEEE-CIS/Vesta)}
\paragraph{Dataset.}
We evaluate on the IEEE-CIS Fraud Detection dataset released as a Kaggle competition, constructed from real-world e-commerce transactions (Vesta) \citep{kaggle_ieee_cis_2019}.
The dataset provides transaction features and (optional) identity/device features joined by \texttt{TransactionID}, with a binary label \texttt{isFraud}.

\paragraph{Chronological splitting.}
Fraud data are time-dependent; to emulate deployment and reduce leakage, we use a chronological split based on \texttt{TransactionDT}:
the earliest segment is used for training, followed by a validation window and a held-out test window.
We use \TODO{e.g., 70/15/15 by time} and report all metrics on the final time window.

\paragraph{Preprocessing.}
We join transaction and identity tables by \texttt{TransactionID}, drop identifiers, and handle missing values with \TODO{strategy}.
Categorical variables are encoded using \TODO{embeddings / target encoding}; numerical variables are standardized \TODO{if needed}.
All preprocessing statistics (e.g., encoders) are fit on the training window only.

\begin{table}[t]
\centering
\caption{Chronological split summary on IEEE-CIS labeled data (fill with your split boundaries and counts).}
\label{tab:data}
\begin{tabular}{lccc}
\toprule
\textbf{Split} & \textbf{TransactionDT range} & \textbf{\# txns} & \textbf{Fraud rate} \\
\midrule
Train & \TODO{} & \TODO{} & \TODO{} \\
Val   & \TODO{} & \TODO{} & \TODO{} \\
Test  & \TODO{} & \TODO{} & \TODO{} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Models and baselines}
\paragraph{Model.}
Our primary model is a \TODO{MLP / wide\&deep / tabular transformer} producing logits $\vz\in\R^K$.
We keep the architecture and optimization budget fixed across losses to isolate the effect of the training objective.

\paragraph{Baselines.}
We compare:
(i) cross-entropy (CE),
(ii) weighted CE,
(iii) focal-style loss,
(iv) CE trained model with post-hoc Bayes action \eqref{eq:bayes_decision} using $\mC$,
(v) \TODO{additional cost-sensitive baselines if used}.
All methods share the same data split, preprocessing, and tuning budget.

\subsection{Cost instantiation and decision policy}
\paragraph{Instance-dependent costs.}
We instantiate per-transaction costs using Table~\ref{tab:cost} with $M=\texttt{TransactionAmt}$.
Concretely,
\[
C_{\text{legit}\rightarrow\text{decline}} = (1+\rho_{\mathrm{FD}})M,
\qquad
C_{\text{fraud}\rightarrow\text{approve}} = \lambda_{\mathrm{cb}} M,
\]
with \TODO{values/ranges} for $\rho_{\mathrm{FD}}$ and $\lambda_{\mathrm{cb}}$.

\paragraph{Decision rule.}
Given predicted distribution $\vp(\vx)$ (or $q(\vz)$ for \CACIS{}), we deploy the Bayes action \eqref{eq:bayes_decision}.
For binary approve/decline, this yields an instance-dependent threshold:
approve iff
\[
p(\text{fraud}\mid \vx) \le \frac{(1+\rho_{\mathrm{FD}})M}{(1+\rho_{\mathrm{FD}})M + \lambda_{\mathrm{cb}} M}
= \frac{1+\rho_{\mathrm{FD}}}{1+\rho_{\mathrm{FD}}+\lambda_{\mathrm{cb}}}.
\]
If you use more elaborate $L_{\mathrm{fraud}}(M)$ (not linear in $M$), the threshold becomes transaction-dependent \TODO{describe if applicable}.

\subsection{Metrics}
\paragraph{Primary: expected regret / profit.}
For each test transaction we compute realized regret using the per-instance costs and the chosen action.
We report mean regret (lower is better) and profit (negative regret) per 1k transactions.
Confidence intervals are estimated using \TODO{bootstrap or time-block bootstrap}.

\paragraph{Secondary metrics.}
We also report AUC-ROC, AUC-PR, approval rate, fraud catch rate, and calibration (ECE/Brier) for diagnostic purposes.

\subsection{Main results}
\begin{table*}[t]
\centering
\caption{Main results on the chronological test window (report mean over \TODO{seeds} with \TODO{CI method}).}
\label{tab:main}
\begin{tabular}{lcccccc}
\toprule
\textbf{Method} & \textbf{Regret $\downarrow$} & \textbf{Profit $\uparrow$} & \textbf{Approval $\uparrow$} & \textbf{Fraud catch $\uparrow$} & \textbf{AUC-PR $\uparrow$} & \textbf{ECE $\downarrow$} \\
\midrule
CE & \TODO{} & \TODO{} & \TODO{} & \TODO{} & \TODO{} & \TODO{} \\
Weighted CE & \TODO{} & \TODO{} & \TODO{} & \TODO{} & \TODO{} & \TODO{} \\
Focal & \TODO{} & \TODO{} & \TODO{} & \TODO{} & \TODO{} & \TODO{} \\
CE + post-hoc policy & \TODO{} & \TODO{} & \TODO{} & \TODO{} & \TODO{} & \TODO{} \\
\CACIS{} (ours) & \textbf{\TODO{}} & \textbf{\TODO{}} & \TODO{} & \TODO{} & \TODO{} & \TODO{} \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{Ablations and sensitivity}
\paragraph{Effect of $\varepsilon$.}
We vary $\varepsilon$ and observe the trade-off between regret optimization and calibration.
Large $\varepsilon$ yields smoother geometry and easier optimization; small $\varepsilon$ approaches sharper OT behavior \citep{cuturi2013sinkhorn, genevay2019sample}.

\paragraph{FW iterations $T$.}
We vary $T$ and report regret and calibration stability, showing diminishing returns beyond \TODO{$T^\star$} iterations.

\begin{table}[t]
\centering
\caption{Ablations for \CACIS{} (fill).}
\label{tab:ablations}
\begin{tabular}{lcc}
\toprule
\textbf{Variant} & \textbf{Regret $\downarrow$} & \textbf{ECE $\downarrow$} \\
\midrule
$\varepsilon=\TODO{}$ & \TODO{} & \TODO{} \\
$\varepsilon=\TODO{}$ & \TODO{} & \TODO{} \\
$T=\TODO{}$ & \TODO{} & \TODO{} \\
$T=\TODO{}$ & \TODO{} & \TODO{} \\
\bottomrule
\end{tabular}
\end{table}

\section{Computational Complexity and Production Suitability}
\label{sec:complexity}

For high-QPS production environments, the training and inference overhead of the loss function is a critical deployment constraint. While standard Optimal Transport (OT) solvers typically scale at $O(K^3 \log K)$, \CACIS{} leverages a specialized optimization path.

\subsection{Frank--Wolfe Efficiency}
The Frank--Wolfe (FW) algorithm used in \CACIS{} (Algorithm~\ref{alg:fw}) provides several advantages for industrial-scale classification:
\begin{itemize}[leftmargin=*]
    \item \textbf{Convergence Rate:} FW achieves an $O(1/T)$ convergence rate on the simplex, where $T$ is the number of iterations. In practice, we observe that $T=10$ iterations are sufficient to stabilize the model-implied distribution $q(\vz)$, making the overhead negligible relative to the backward pass of a deep neural network.
    \item \textbf{Memory Footprint:} Unlike Sinkhorn iterations that require storing scaling vectors $\mathbf{u}, \mathbf{v}$ and potentially large Gibbs kernels $\mK$, the FW linear oracle only requires the gradient of the quadratic form $\mM\valpha$, which is memory-efficient for large label sets.
    \item \textbf{Numerical Stability:} By operating directly on the simplex $\Delta^K$, FW avoids the "log-sum-exp" overflows often encountered in unconstrained logit parameterizations, ensuring stable gradients throughout long-running production training jobs.
\end{itemize}

\section{Instance-Dependent Cost Logic}
\label{sec:cost_logic}

In e-commerce fraud detection, treating costs as static class weights fails to capture the risk associated with transaction variance. We define instance-dependent costs $C_{ij}(\vx)$ as follows:

\subsection{Chargeback Severity ($\lambda_{cb}$)}
The cost of a False Negative (missed fraud) is not merely the transaction amount $M$, but the total "Regret" of approval. We define the fraud loss multiplier:
\begin{equation}
    L_{\mathrm{fraud}}(M) = \lambda_{cb} \cdot M + F_{cb}
\end{equation}
where $M$ is the feature \texttt{TransactionAmt}, $F_{cb}$ represents fixed dispute fees (typically \$15--\$25), and $\lambda_{cb} \in [1.5, 3.0]$ accounts for the "True Cost of Fraud," including shipping, lost inventory, and overhead.

\subsection{False Decline Friction ($\rho_{FD}$)}
A False Positive (declining a legitimate user) destroys the current margin $M \cdot m$ and risks future customer lifetime value (LTV). We define:
\begin{equation}
    C_{\text{legit}\rightarrow\text{decline}} = (1 + \rho_{FD}) \cdot (M \cdot m)
\end{equation}
where $\rho_{FD}$ is the churn factor. In high-growth e-commerce, $\rho_{FD}$ is often set between $0.2$ and $0.5$ to reflect the high cost of customer acquisition.

\section{Experimental Evaluation}
\TODO{Insert Table 3: Performance Comparison. Rows: CE, Weighted CE, CACIS. Columns: AUC-ROC, ECE (Calibration), Total Fraud Loss Avoided (\$), Total False Decline Friction (\$), and Net Business Value (\$).}

\TODO{Insert Figure 4: Calibration Curves. Plot reliability diagrams for CE and CACIS to prove that CACIS maintains probabilistic fidelity despite its cost-focus.}

\TODO{Insert Figure 5: Sensitivity Analysis. Vary $\rho_{FD}$ from 0.0 to 1.0 on the x-axis and plot Total Regret on the y-axis to show CACIS's robustness to business parameter shifts.}

\TODO{Add Section 5.7: Deployment Metrics. Report P95 Latency of Algorithm 2 and the 'Shadow Mode' results comparing CACIS's expected savings against the current production baseline.}

% =========================================================
% 6. PRACTICAL GUIDELINES
% =========================================================
\section{Practical Guidelines}
\paragraph{G1: Start from value, not labels.}
Define a value function over (truth, action) and convert it to regret so that zero cost corresponds to the best attainable decision \citep{berger1985statistical}.

\paragraph{G2: Make costs comparable and stable.}
Express costs in meaningful units (e.g., dollars per transaction), clip extremes, and audit sensitivity.
A practical rule is to scale costs so that the median non-zero entry is $1$ and tune $\varepsilon$ around that scale.

\paragraph{G3: Separate modeling from policy, but train for the policy.}
Deployment uses $\argmin_j r(j\mid \vp)$; training should shape $\vp$ to reflect costs, not merely rank classes.

\paragraph{G4: Tune $\varepsilon$ as a smoothness knob.}
Large $\varepsilon$ yields smoother geometry and easier optimization; small $\varepsilon$ approaches sharper OT behavior \citep{cuturi2013sinkhorn, genevay2019sample}.

\paragraph{G5: Report decision metrics first.}
Lead with regret/profit under the deployed policy; report AUC/accuracy for comparability.

% =========================================================
% 7. LIMITATIONS + BROADER IMPACT
% =========================================================
\section{Limitations and Broader Impact}
\paragraph{Limitations.}
\begin{itemize}[leftmargin=*]
\item \textbf{Cost specification risk.} If $\mC$ is misspecified, training optimizes the wrong geometry. We recommend sensitivity analyses over $\rho_{\mathrm{FD}}$ and $\lambda_{\mathrm{cb}}$.
\item \textbf{Benchmark constraints.} IEEE-CIS provides binary labels and does not expose true chargeback timing; real production settings may involve delayed/noisy supervision and multi-action policies.
\item \textbf{Computation for large $K$.} The naive inner loop scales as $O(TK^2)$; our setting is binary, but extensions to large label/action sets motivate structured approximations.
\end{itemize}

\paragraph{Broader impact.}
Cost-aware fraud prevention can reduce economic losses and improve user experience when costs reflect real harm.
However, misspecified costs may amplify unfair outcomes; we recommend segment-level audits and transparency about the cost model.

% =========================================================
% 8. CONCLUSION
% =========================================================
\section{Conclusion}
We framed cost-sensitive fraud detection as decision-making under uncertainty and showed how costs induce a meaningful geometry over labels.
We introduced \CACIS{}, a cost-aware loss built from entropic OT and optimized via a simplex-stable inner loop.
On the IEEE-CIS/Vesta benchmark \citep{kaggle_ieee_cis_2019}, \CACIS{} improves decision-aligned regret relative to decision-agnostic objectives, while maintaining strong AUC and calibration.

\appendix
\section{Additional derivations and implementation details}
\TODO{Optionally add: explicit $q(\vz)$ computation; hyperparameter grids; preprocessing details; extra ablations.}

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
